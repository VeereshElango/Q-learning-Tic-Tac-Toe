{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "tic-tac-toe.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VeereshElango/Q-learning-Tic-Tac-Toe/blob/main/tic-tac-toe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ_mtRtZcvVd"
      },
      "source": [
        "# An AI agent learns to play Tic Tac Toe\n",
        "---\n",
        "Two players against each other\n",
        "\n",
        "<img style=\"float:left\" src=\"https://github.com/VeereshElango/Q-learning-Tic-Tac-Toe/blob/main/images/board.png?raw=1\" alt=\"drawing\" width=\"200\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oi1PIZsYitl7"
      },
      "source": [
        "###Overview of the soluton\n",
        "\n",
        "\n",
        "<img style=\"float:left\" src=\"https://github.com/VeereshElango/Q-learning-Tic-Tac-Toe/blob/main/images/overview.PNG?raw=1\" alt=\"drawing\" width=\"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEs981gHcvVe"
      },
      "source": [
        "Importing python libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rii3PqV8cvVe"
      },
      "source": [
        "import numpy as np\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ulYTxU0cvVf"
      },
      "source": [
        "BOARD_ROWS = 3\n",
        "BOARD_COLS = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5Z7B126cvVg"
      },
      "source": [
        "### Board State\n",
        "---\n",
        "Defining the structure of the board of the tic tac toe game. This is where define the environment where the agent interacts.\n",
        "\n",
        "- 2 players p1 and p2\n",
        "- p1 uses symbol 1 \n",
        "- p2 uses symbol 2\n",
        "- vacancy as 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T62amil1cvVh"
      },
      "source": [
        "class State:\n",
        "    def __init__(self, p1, p2):\n",
        "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
        "        self.p1 = p1\n",
        "        self.p2 = p2\n",
        "        self.isEnd = False\n",
        "        self.boardHash = None\n",
        "        # init p1 plays first\n",
        "        self.playerSymbol = 1\n",
        "    \n",
        "    # get unique hash of current board state\n",
        "    def getHash(self):\n",
        "        self.boardHash = str(self.board.reshape(BOARD_COLS*BOARD_ROWS))\n",
        "        return self.boardHash\n",
        "    \n",
        "    def winner(self):\n",
        "        # row\n",
        "        for i in range(BOARD_ROWS):\n",
        "            if sum(self.board[i, :]) == 3:\n",
        "                self.isEnd = True\n",
        "                return 1\n",
        "            if sum(self.board[i, :]) == -3:\n",
        "                self.isEnd = True\n",
        "                return -1\n",
        "        # col\n",
        "        for i in range(BOARD_COLS):\n",
        "            if sum(self.board[:, i]) == 3:\n",
        "                self.isEnd = True\n",
        "                return 1\n",
        "            if sum(self.board[:, i]) == -3:\n",
        "                self.isEnd = True\n",
        "                return -1\n",
        "        # diagonal\n",
        "        diag_sum1 = sum([self.board[i, i] for i in range(BOARD_COLS)])\n",
        "        diag_sum2 = sum([self.board[i, BOARD_COLS-i-1] for i in range(BOARD_COLS)])\n",
        "        diag_sum = max(diag_sum1, diag_sum2)\n",
        "        if diag_sum == 3:\n",
        "            self.isEnd = True\n",
        "            return 1\n",
        "        if diag_sum == -3:\n",
        "            self.isEnd = True\n",
        "            return -1\n",
        "        \n",
        "        # tie\n",
        "        # no available positions\n",
        "        if len(self.availablePositions()) == 0:\n",
        "            self.isEnd = True\n",
        "            return 0\n",
        "        # not end\n",
        "        self.isEnd = False\n",
        "        return None\n",
        "    \n",
        "    def availablePositions(self):\n",
        "        positions = []\n",
        "        for i in range(BOARD_ROWS):\n",
        "            for j in range(BOARD_COLS):\n",
        "                if self.board[i, j] == 0:\n",
        "                    positions.append((i, j))  # need to be tuple\n",
        "        return positions\n",
        "    \n",
        "    def updateState(self, position):\n",
        "        self.board[position] = self.playerSymbol\n",
        "        # switch to another player\n",
        "        self.playerSymbol = -1 if self.playerSymbol == 1 else 1\n",
        "    \n",
        "    # only when game ends\n",
        "    def giveReward(self):\n",
        "        result = self.winner()\n",
        "        # backpropagate reward\n",
        "        if result == 1:\n",
        "            self.p1.feedReward(1)\n",
        "            self.p2.feedReward(0)\n",
        "        elif result == -1:\n",
        "            self.p1.feedReward(0)\n",
        "            self.p2.feedReward(1)\n",
        "        else:\n",
        "            self.p1.feedReward(0.1)\n",
        "            self.p2.feedReward(0.5)\n",
        "    \n",
        "    # board reset\n",
        "    def reset(self):\n",
        "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
        "        self.boardHash = None\n",
        "        self.isEnd = False\n",
        "        self.playerSymbol = 1\n",
        "    \n",
        "    def play(self, rounds=100):\n",
        "        for i in range(rounds):\n",
        "            if i%1000 == 0:\n",
        "                print(\"Rounds {}\".format(i))\n",
        "            while not self.isEnd:\n",
        "                # Player 1\n",
        "                positions = self.availablePositions()\n",
        "                p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)\n",
        "                # take action and upate board state\n",
        "                self.updateState(p1_action)\n",
        "                board_hash = self.getHash()\n",
        "                self.p1.addState(board_hash)\n",
        "                # check board status if it is end\n",
        "\n",
        "                win = self.winner()\n",
        "                if win is not None:\n",
        "                    # self.showBoard()\n",
        "                    # ended with p1 either win or draw\n",
        "                    self.giveReward()\n",
        "                    self.p1.reset()\n",
        "                    self.p2.reset()\n",
        "                    self.reset()\n",
        "                    break\n",
        "\n",
        "                else:\n",
        "                    # Player 2\n",
        "                    positions = self.availablePositions()\n",
        "                    p2_action = self.p2.chooseAction(positions, self.board, self.playerSymbol)\n",
        "                    self.updateState(p2_action)\n",
        "                    board_hash = self.getHash()\n",
        "                    self.p2.addState(board_hash)\n",
        "                    \n",
        "                    win = self.winner()\n",
        "                    if win is not None:\n",
        "                        # self.showBoard()\n",
        "                        # ended with p2 either win or draw\n",
        "                        self.giveReward()\n",
        "                        self.p1.reset()\n",
        "                        self.p2.reset()\n",
        "                        self.reset()\n",
        "                        break\n",
        "    \n",
        "    # play with human\n",
        "    def play2(self):\n",
        "        while not self.isEnd:\n",
        "            # Player 1\n",
        "            positions = self.availablePositions()\n",
        "            p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)\n",
        "            # take action and upate board state\n",
        "            self.updateState(p1_action)\n",
        "            self.showBoard()\n",
        "            # check board status if it is end\n",
        "            win = self.winner()\n",
        "            if win is not None:\n",
        "                if win == 1:\n",
        "                    print(self.p1.name, \"wins!\")\n",
        "                else:\n",
        "                    print(\"tie!\")\n",
        "                self.reset()\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                # Player 2\n",
        "                positions = self.availablePositions()\n",
        "                p2_action = self.p2.chooseAction(positions)\n",
        "\n",
        "                self.updateState(p2_action)\n",
        "                self.showBoard()\n",
        "                win = self.winner()\n",
        "                if win is not None:\n",
        "                    if win == -1:\n",
        "                        print(self.p2.name, \"wins!\")\n",
        "                    else:\n",
        "                        print(\"tie!\")\n",
        "                    self.reset()\n",
        "                    break\n",
        "\n",
        "    def showBoard(self):\n",
        "        # p1: x  p2: o\n",
        "        for i in range(0, BOARD_ROWS):\n",
        "            print('-------------')\n",
        "            out = '| '\n",
        "            for j in range(0, BOARD_COLS):\n",
        "                if self.board[i, j] == 1:\n",
        "                    token = 'x'\n",
        "                if self.board[i, j] == -1:\n",
        "                    token = 'o'\n",
        "                if self.board[i, j] == 0:\n",
        "                    token = ' '\n",
        "                out += token + ' | '\n",
        "            print(out)\n",
        "        print('-------------')    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goMBhcBbcvVl"
      },
      "source": [
        "### AI Player\n",
        "---\n",
        "Defining the player for tic tac toe game and its possible actions. This is where define the agent and its potential action to perform within the above defined environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JotRAr0lcvVn"
      },
      "source": [
        "class Player:\n",
        "    def __init__(self, name, exp_rate=0.3):\n",
        "        self.name = name\n",
        "        self.states = []  # record all positions taken\n",
        "        self.lr = 0.2\n",
        "        self.exp_rate = exp_rate\n",
        "        self.decay_gamma = 0.9\n",
        "        self.states_value = {}  # state -> value\n",
        "    \n",
        "    def getHash(self, board):\n",
        "        boardHash = str(board.reshape(BOARD_COLS*BOARD_ROWS))\n",
        "        return boardHash\n",
        "    \n",
        "    def chooseAction(self, positions, current_board, symbol):\n",
        "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
        "            # take random action\n",
        "            idx = np.random.choice(len(positions))\n",
        "            action = positions[idx]\n",
        "        else:\n",
        "            value_max = -999\n",
        "            for p in positions:\n",
        "                next_board = current_board.copy()\n",
        "                next_board[p] = symbol\n",
        "                next_boardHash = self.getHash(next_board)\n",
        "                value = 0 if self.states_value.get(next_boardHash) is None else self.states_value.get(next_boardHash)\n",
        "                # print(\"value\", value)\n",
        "                if value >= value_max:\n",
        "                    value_max = value\n",
        "                    action = p\n",
        "        # print(\"{} takes action {}\".format(self.name, action))\n",
        "        return action\n",
        "    \n",
        "    # append a hash state\n",
        "    def addState(self, state):\n",
        "        self.states.append(state)\n",
        "    \n",
        "    # at the end of game, backpropagate and update states value\n",
        "    def feedReward(self, reward):\n",
        "        for st in reversed(self.states):\n",
        "            if self.states_value.get(st) is None:\n",
        "                self.states_value[st] = 0\n",
        "            self.states_value[st] += self.lr*(self.decay_gamma*reward - self.states_value[st])\n",
        "            reward = self.states_value[st]\n",
        "            \n",
        "    def reset(self):\n",
        "        self.states = []\n",
        "        \n",
        "    def savePolicy(self):\n",
        "        fw = open('policy_' + str(self.name), 'wb')\n",
        "        pickle.dump(self.states_value, fw)\n",
        "        fw.close()\n",
        "\n",
        "    def loadPolicy(self, file):\n",
        "        fr = open(file,'rb')\n",
        "        self.states_value = pickle.load(fr)\n",
        "        fr.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IVTBuMpcvVo"
      },
      "source": [
        "### Human Player\n",
        "---\n",
        "Defining the human player for tic tac toe game and its possible actions. This human player will be used to play agains the AI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKXvG50BcvVo"
      },
      "source": [
        "class HumanPlayer:\n",
        "    def __init__(self, name):\n",
        "        self.name = name \n",
        "    \n",
        "    def chooseAction(self, positions):\n",
        "        while True:\n",
        "            row = int(input(\"Input your action row:\"))\n",
        "            col = int(input(\"Input your action col:\"))\n",
        "            action = (row, col)\n",
        "            if action in positions:\n",
        "                return action\n",
        "    \n",
        "    # append a hash state\n",
        "    def addState(self, state):\n",
        "        pass\n",
        "    \n",
        "    # at the end of game, backpropagate and update states value\n",
        "    def feedReward(self, reward):\n",
        "        pass\n",
        "            \n",
        "    def reset(self):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0Zk-3HNcvVo"
      },
      "source": [
        "### Training the AI "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9OYkYmGkcvVp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed527952-2bd4-4ec0-92cf-ee9f787f83d2"
      },
      "source": [
        "p1 = Player(\"p1\")\n",
        "p2 = Player(\"p2\")\n",
        "\n",
        "st = State(p1, p2)\n",
        "print(\"training...\")\n",
        "st.play(50000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training...\n",
            "Rounds 0\n",
            "Rounds 1000\n",
            "Rounds 2000\n",
            "Rounds 3000\n",
            "Rounds 4000\n",
            "Rounds 5000\n",
            "Rounds 6000\n",
            "Rounds 7000\n",
            "Rounds 8000\n",
            "Rounds 9000\n",
            "Rounds 10000\n",
            "Rounds 11000\n",
            "Rounds 12000\n",
            "Rounds 13000\n",
            "Rounds 14000\n",
            "Rounds 15000\n",
            "Rounds 16000\n",
            "Rounds 17000\n",
            "Rounds 18000\n",
            "Rounds 19000\n",
            "Rounds 20000\n",
            "Rounds 21000\n",
            "Rounds 22000\n",
            "Rounds 23000\n",
            "Rounds 24000\n",
            "Rounds 25000\n",
            "Rounds 26000\n",
            "Rounds 27000\n",
            "Rounds 28000\n",
            "Rounds 29000\n",
            "Rounds 30000\n",
            "Rounds 31000\n",
            "Rounds 32000\n",
            "Rounds 33000\n",
            "Rounds 34000\n",
            "Rounds 35000\n",
            "Rounds 36000\n",
            "Rounds 37000\n",
            "Rounds 38000\n",
            "Rounds 39000\n",
            "Rounds 40000\n",
            "Rounds 41000\n",
            "Rounds 42000\n",
            "Rounds 43000\n",
            "Rounds 44000\n",
            "Rounds 45000\n",
            "Rounds 46000\n",
            "Rounds 47000\n",
            "Rounds 48000\n",
            "Rounds 49000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXAmmKi4cvVq"
      },
      "source": [
        "Save the trained AI player"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6Ew19e_cvVr"
      },
      "source": [
        "p1.savePolicy()\n",
        "p2.savePolicy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt_p20OicvVr"
      },
      "source": [
        "Load the trained AI player"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5ivLouOcvVr"
      },
      "source": [
        "p1.loadPolicy(\"policy_p1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRO_qZkPcvVr"
      },
      "source": [
        "### Human vs Computer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLgIvhkv-6Hr"
      },
      "source": [
        "A reference on how to feed the positions when you run below code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5w5mDi6-wh_"
      },
      "source": [
        "<img style=\"float:left\" src=\"https://github.com/VeereshElango/Q-learning-Tic-Tac-Toe/blob/main/images/matrix.PNG?raw=1\" alt=\"drawing\" width=\"200\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jzg6NUJcvVs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6fbc64e-39c2-4948-b7e3-ad391ca43f9d"
      },
      "source": [
        "p1 = Player(\"computer\", exp_rate=0)\n",
        "p1.loadPolicy(\"policy_p1\")\n",
        "\n",
        "p2 = HumanPlayer(\"human\")\n",
        "\n",
        "st = State(p1, p2)\n",
        "st.play2()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "| x |   |   | \n",
            "-------------\n",
            "Input your action row:1\n",
            "Input your action col:1\n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   | o |   | \n",
            "-------------\n",
            "| x |   |   | \n",
            "-------------\n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   | o |   | \n",
            "-------------\n",
            "| x |   | x | \n",
            "-------------\n",
            "Input your action row:2\n",
            "Input your action col:1\n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   | o |   | \n",
            "-------------\n",
            "| x | o | x | \n",
            "-------------\n",
            "-------------\n",
            "|   | x |   | \n",
            "-------------\n",
            "|   | o |   | \n",
            "-------------\n",
            "| x | o | x | \n",
            "-------------\n",
            "Input your action row:1\n",
            "Input your action col:0\n",
            "-------------\n",
            "|   | x |   | \n",
            "-------------\n",
            "| o | o |   | \n",
            "-------------\n",
            "| x | o | x | \n",
            "-------------\n",
            "-------------\n",
            "|   | x |   | \n",
            "-------------\n",
            "| o | o | x | \n",
            "-------------\n",
            "| x | o | x | \n",
            "-------------\n",
            "Input your action row:0\n",
            "Input your action col:0\n",
            "-------------\n",
            "| o | x |   | \n",
            "-------------\n",
            "| o | o | x | \n",
            "-------------\n",
            "| x | o | x | \n",
            "-------------\n",
            "-------------\n",
            "| o | x | x | \n",
            "-------------\n",
            "| o | o | x | \n",
            "-------------\n",
            "| x | o | x | \n",
            "-------------\n",
            "computer wins!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nkK9jAWcvVs"
      },
      "source": [
        "p1 = Player(\"computer\", exp_rate=0)\n",
        "p1.loadPolicy(\"policy_p1\")\n",
        "\n",
        "p2 = HumanPlayer(\"human\")\n",
        "\n",
        "st = State(p1, p2)\n",
        "st.play2()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k3NBGOzdS20"
      },
      "source": [
        "###Credits\n",
        "---\n",
        "This code is a mere wrapper of [Jermey Zhang](https://github.com/MJeremy2017/reinforcement-learning-implementation/tree/master/TicTacToe). A huge shout out for his amazing work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8GOTOkbd1BN"
      },
      "source": [
        "###References\n",
        "\n",
        "\n",
        "*   [Introduction to Reinforcement Learning by Sutton and Barto](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)\n",
        "*   [Classic to State of Art methods on Reinforcement learning by Carsten Friedrich](https://github.com/fcarsten/tic-tac-toe) \n",
        "*   [Visualization of value Function](https://jinglescode.github.io/reinforcement-learning-tic-tac-toe/)\n",
        "*   [Video explanation](https://youtu.be/id66dboTCKI)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zslBc1_ifAzW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}